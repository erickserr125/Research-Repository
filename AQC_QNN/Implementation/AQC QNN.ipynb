{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eecc5d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "FOR INFORMATION REGARDING THE FIRST THREE CODE CELLS, REFER TO THE PRELIMINARY WORK (AQC SIMULATION\n",
    "AND COMPLEXODE SAMPLE)\n",
    "\"\"\"\n",
    "\n",
    "from scipy.integrate import complex_ode\n",
    "\"\"\"\n",
    "Before we continue, we note that the complex_ode does not accept \n",
    "additional arguments using the function .set_f_params()\n",
    "\n",
    "For whatever reason, scipy.intergate.complex_ode does not \n",
    "pass extra arguments. Instead we will create a class that, \n",
    "by default, passes the extra arguments into the function.\n",
    "\"\"\"\n",
    "\n",
    "class FuncObj(object):\n",
    "    def __init__(self, f, farg):\n",
    "        self._ode_ham = f\n",
    "        self.farg=farg\n",
    "\n",
    "    def ode_ham(self, t, y):\n",
    "        return self._ode_ham(t, y, self.farg)\n",
    "    \n",
    "    def set_arg(self, farg):\n",
    "        self.farg = farg\n",
    "        return\n",
    "    def print_fargs(self):\n",
    "        print(\"fargs:\", self.farg)\n",
    "        print(\"fargs.shape: \", self.farg.shape)\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2657e7dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ode_ham(t,psi,ham):\n",
    "    \"\"\"\n",
    "    ODE which represents the formula\n",
    "\n",
    "    dc_j                                     \n",
    "    ---- = sum_k(<j|H_t|k> c_k)                        \n",
    "     dt\n",
    "\n",
    "    In other words, for all states of psi, summed by the equation\n",
    "     sum_k(H_t|k>c_k)\n",
    "    We also distribute the inner product with the bra vector <j|,\n",
    "    Where j\n",
    "    \"\"\"\n",
    "    #print(\"Time slice: \", t)\n",
    "    #print(\"Psi: \", psi)\n",
    "    #print(\"Current Hamiltonian: \", ham)\n",
    "    coeff_derivatives = np.empty((0,0))\n",
    "    psi = psi.reshape((psi.shape[0],1))\n",
    "    \n",
    "    #print(\"Time slice: \", t)\n",
    "    #For all basis vectors 2^n, |j> at time, t:\n",
    "    #Or is it, for all vectors in |psi>, c_j|j>, at time, t?\n",
    "    for j in range(psi.shape[0]):\n",
    "        state_j = np.zeros((psi.shape[0],1))\n",
    "        #Complex conjugate transpose:\n",
    "        #(0..... 1 .....0)\n",
    "        state_j[j] = 1\n",
    "        state_j = np.matrix.getH(state_j)\n",
    "        #print(\"New state |j>: \", state_j)\n",
    "        \n",
    "        #Calculates <j|H_t|k>c_k\n",
    "        #For all c_k in psi\n",
    "        c = -1j*np.dot(state_j,np.dot(ham,psi))\n",
    "\n",
    "        #Print only if we have few time slices, t\n",
    "        #print(\"Coefficient dc_\",j,\"/dt:\", c)\n",
    "        #The R.H.S. of the ODE for c_j at time, t\n",
    "        #print(\"dC_j/dt: \", c)\n",
    "        coeff_derivatives = np.append(coeff_derivatives,c)\n",
    "        #print(\"Shape of coefficient list dc_i/dt:\",\n",
    "        #coeff_derivatives.shape)\n",
    "        \n",
    "        \n",
    "    #R.H.S. of ODE of c_j for ALL states j in |psi(t)> \n",
    "    return coeff_derivatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bebfe2b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve_comp(ode_solver, H_t, psi_0, t):\n",
    "    arg = FuncObj(ode_solver, H_t[:,:,0])\n",
    "\n",
    "    \"\"\"\n",
    "    Create our complex_ode object for H_t1\n",
    "    \"\"\"\n",
    "    r = complex_ode(arg.ode_ham)\n",
    "    r.set_integrator(\"dop853\")\n",
    "    r.set_initial_value(psi_0,t[0])\n",
    "    \"\"\"\n",
    "    Stores eiegenvalues of size psi_0.shape[0] for \n",
    "    t.shape[0] time intervals\n",
    "    \"\"\"\n",
    "    psi_t = np.empty((psi_0.shape[0],t.shape[0]))\n",
    "    #print(\"psi_t.shape: \",psi_t.shape)\n",
    "\n",
    "    #For every time slice, integrate the complex ode\n",
    "    for time in range(t.shape[0]):\n",
    "        arg.set_arg(H_t[:,:,time])\n",
    "        r.integrate(r.t+dt)\n",
    "        #print(H_t[time,:,:].shape)\n",
    "        #print(\"New state coefficients - \",r.y)\n",
    "        psi_f = r.y\n",
    "        #print(psi_f.shape)    \n",
    "        psi_t[:,time] = psi_f.copy()\n",
    "        #print(\"New Coefficients - translated: \",psi_t[:,time])\n",
    "\n",
    "    #psi_t to see evolution, \n",
    "    #psi_f to see final state and use for calculations\n",
    "    return psi_t, psi_f"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19527ff2",
   "metadata": {},
   "source": [
    "# Put it into practice\n",
    "\n",
    "We recognize we can find the ground state of a hamiltonian via the AQC algorithm; another way of looking at this is the minimization of the cost function. Let us instead consider the following cost function model for a neural network for $K$ training examples:\n",
    "\n",
    "$$\n",
    "-C = (1/size) *\\sum^K_{m=1} Y^mlog(A^m) + (1-Y^m) log(1-A^m)\n",
    "$$\n",
    "\n",
    "for every input feature $x_i$ and $Y$ is constant for a given training example $x^l$ for all training examples $m$. In addition, we will consider the following functions and restrictions (for a single training example) for the feed-forward neural network:\n",
    "\n",
    "$$\n",
    "f^m = \\sum^n_{i=1}w_i x_i,\n",
    "$$\n",
    "$$\n",
    "A^m = \\frac{1}{1+e^{-f}}\n",
    "$$\n",
    "and of course\n",
    "$$\n",
    "x_i \\in \\{-1,1\\}\\quad y_i\\in \\{0,1\\}\\quad A_i\\in [0,1]\\quad f_i \\in \\mathcal{R} \\quad w_i\\in \\mathcal{R}\n",
    "$$\n",
    "It should be noted that cases where $y_i=A_i$ lead to an undefined cost function. We take care of this in our neural network function by reducing the cost for the training example to $0$, since there is zero error.\n",
    "\n",
    "We are restricted to a stream of binary input data, and it is labelled as $0$ or $1$ as dictated by $y$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e23770c",
   "metadata": {},
   "source": [
    "Our expected neural network resembles a `perceptron`(zero hidden layers) in which we depend solely on the value of the output layer to derive our `gradients`, or the derivative of the cost function with respect to $w$:\n",
    "\n",
    "$$\n",
    "\\frac{d C}{dw_i}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "513d3a4d",
   "metadata": {},
   "source": [
    "We can analytically solve for this gradient by applying the chain rule\n",
    "\n",
    "$$\n",
    "\\frac{\\partial C_i}{\\partial w_i} = \\frac{\\partial C_i}{\\partial A_i}\\frac{\\partial A_i}{\\partial w_i} = \\frac{\\partial C_i}{\\partial A_i}\\frac{\\partial A_i}{\\partial f_i}\\frac{\\partial f_i}{\\partial w_i}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc440ab7",
   "metadata": {},
   "source": [
    "We find that \n",
    "\n",
    "$$\n",
    "\\frac{\\partial C_i}{\\partial A_i} = \\frac{Y}{A_i} - \\frac{1-Y}{1-A_i} = \\frac{y-A_i}{A_i(1-A_i)} \n",
    "$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\n",
    "\\frac{\\partial A_i}{\\partial f_i} = A_i(1-A_i) \n",
    "$$\n",
    "\n",
    "and\n",
    "$$\n",
    "\\frac{\\partial f_i}{\\partial w_i} = x_i\n",
    "$$\n",
    "\n",
    "\n",
    "### Final Gradient Answer\n",
    "So our final answer is:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial C_i}{\\partial w_i} = (y-A_i)x_i\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb7f0dfa",
   "metadata": {},
   "source": [
    "This is the function we hope to describe in our **Cost Hamiltonian**. Allow us to consider the following translations for each element operating on the $i-th$ qubit:\n",
    "$$\n",
    "x_i\\rightarrow Z\n",
    "$$\n",
    "since $\\lambda_Z = \\pm1$ will give us non-zero eigenvalues for $x_i$. If $x_i\\in \\{0,1\\}$, there would be no gradient for $w_i$ when it's associated feature is $x_i=0$ as we can see by the gradient equation above. Our label transitions to:\n",
    "\n",
    "$$\n",
    "Y\\rightarrow \\mathbb{1},\\quad \\mathbf{0}\n",
    "$$\n",
    "\n",
    "since $Y$ is not dependent on $X$, and\n",
    "\n",
    "\n",
    "$$A\\rightarrow H_A = \n",
    "\\begin{pmatrix}\n",
    "A & 0 \\\\\n",
    "0 & -A\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "since $\\lambda_A = \\pm \\frac{1}{1+e^{-w_ix_i}}$, where we can easily determine the ground-state in the negative range (and simply change the sign/polarity if $y\\equiv x$ in value)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5810d4fd",
   "metadata": {},
   "source": [
    "Thus, our gradient hamiltonian emerges as \n",
    "\n",
    "$$\n",
    "H_{\\partial w_i} = [Y - H_i] Z_i\n",
    "$$\n",
    "\n",
    "or, more succinctly \n",
    "\n",
    " $$ H_{\\partial w_i} =   \\left\\{\n",
    "\\begin{array}{ll}\n",
    "      [\\mathbb{1} - H_i] Z_i& Y= \\mathbb{1} \\\\\n",
    "      -H_i Z_i& Y = \\mathbf{0} \\\\\n",
    "\\end{array} \n",
    "\\right.  $$\n",
    "\n",
    "for a weight. If we want to use multiple, we simply apply AQC for every $H_{\\partial i}$ for $n$ features total"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a1f2273",
   "metadata": {},
   "source": [
    "Note that we will use AQC to find the minimum value of $\\frac{\\partial J}{\\partial w_i}$; it might be easier to find the global minima of the total cost function by attempting to find the minimum of the smaller cost functions. We consider the following training data(and it can really be changed to *any* type of data and still work!):\n",
    "    \n",
    "|$x_1$,$x_2$,$x_3$,$x_4$,$x_5$ | $y$|\n",
    "|---------|--------|\n",
    "|1,X,X,X,X| 1|\n",
    "|1,1,1,1,1| 0|\n",
    "\n",
    "Otherwise, the function yields $0$. There are $2^5 = 32$ possible combinations to consider as training data. Allow us to construct this data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "af3bb0e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Training Features,Training Examples):  (5, 32)\n",
      "[[-1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.  1.  1.\n",
      "   1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]\n",
      " [-1. -1. -1. -1. -1. -1. -1. -1.  1.  1.  1.  1.  1.  1.  1.  1. -1. -1.\n",
      "  -1. -1. -1. -1. -1. -1.  1.  1.  1.  1.  1.  1.  1.  1.]\n",
      " [-1. -1. -1. -1.  1.  1.  1.  1. -1. -1. -1. -1.  1.  1.  1.  1. -1. -1.\n",
      "  -1. -1.  1.  1.  1.  1. -1. -1. -1. -1.  1.  1.  1.  1.]\n",
      " [-1.  1.  1. -1. -1.  1.  1. -1. -1.  1.  1. -1. -1.  1.  1. -1. -1.  1.\n",
      "   1. -1. -1.  1.  1. -1. -1.  1.  1. -1. -1.  1.  1. -1.]\n",
      " [-1.  1. -1.  1. -1.  1. -1.  1. -1.  1. -1.  1. -1.  1. -1.  1. -1.  1.\n",
      "  -1.  1. -1.  1. -1.  1. -1.  1. -1.  1. -1.  1. -1.  1.]]\n",
      "Number of training labels (1, 32)\n",
      "Labels:  [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      "  1. 1. 1. 1. 1. 1. 1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "x = np.ones((5,2**5))\n",
    "print(\"(Training Features,Training Examples): \",x.shape)\n",
    "\n",
    "#Every combination of 5 bits hardcoded into a 5X32 array\n",
    "#We will convert zero to -1 so that we can reverse the polarity when we add \\partial w\n",
    "# to w; in other words w_i \\pm =\\partial w_i \n",
    "x[0] = -1\n",
    "x[0,16:32]= 1\n",
    "x[1] = -1\n",
    "x[1,8:16] = 1\n",
    "x[1,24:32] = 1\n",
    "x[2] = -1\n",
    "x[2,4::8] = 1\n",
    "x[2,5::8] = 1\n",
    "x[2,6::8] = 1\n",
    "x[2,7::8] = 1\n",
    "x[3] = -1\n",
    "x[3,1::4] = 1\n",
    "x[3,2::4] = 1\n",
    "x[4,0::2] = -1\n",
    "x[4,1::2] = 1\n",
    "print(x)\n",
    "\n",
    "y = np.zeros((1,2**5))\n",
    "y[0,16:31] = 1\n",
    "print(\"Number of training labels\", y.shape)\n",
    "\n",
    "print(\"Labels: \",y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ff0f138",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1  0]\n",
      " [ 0 -1]]\n",
      "(2, 2, 1)\n",
      "(2,)\n",
      "Initial Ground State:  [0. 1.]\n"
     ]
    }
   ],
   "source": [
    "hw = 1\n",
    "#Pauli_z-gate\n",
    "H_0 = hw* np.array([[1,0],[0,-1]])\n",
    "print(H_0)\n",
    "H_0 = H_0.reshape((H_0.shape[0],H_0.shape[1],1))\n",
    "print(H_0.shape)\n",
    "psi_0 = np.zeros((2**(H_0.shape[0]-1),))\n",
    "print(psi_0.shape)\n",
    "psi_0[H_0.shape[0]-1] = 1\n",
    "print(\"Initial Ground State: \",psi_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "700c4641",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100,)\n",
      "dt =  0.20202020202020202\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "NOTE: In classical simulation $T$ does not determine\n",
    "the accuracy of the expectation value, $t$ does. This is because\n",
    "$T$\n",
    "\"\"\"\n",
    "\n",
    "T = 20 #End at the number 20(assume seconds)\n",
    "slices = 100\n",
    "t = np.linspace(0,T,slices)\n",
    "print(t.shape)\n",
    "dt = t[1] - t[0] #Evenly spaced time intervals\n",
    "print(\"dt = \",dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7d8b3a09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight shape: (1, 5)\n",
      "Weight Values: [[ 0.98031757 -0.56885227  0.03363721 -1.57479618 -1.75170293]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-3-0cd1e652956c>:25: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "  psi_t[:,time] = psi_f.copy()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight values:  [[ 2.06982384 -0.65705401 -0.17074296 -0.21172385 -1.96765415]]\n",
      "Current Cost 0.26479169703425653\n",
      "Weight values:  [[ 2.49858759 -0.70316779 -0.24813959  0.47701789 -2.15875762]]\n",
      "Current Cost 0.23631339471722573\n",
      "Weight values:  [[ 2.96733888 -0.65230159 -0.22469402  0.9874089  -2.27912103]]\n",
      "Current Cost 0.20687075856209913\n",
      "Weight values:  [[ 3.38639817 -0.61777026 -0.20481726  1.35379132 -2.40231501]]\n",
      "Current Cost 0.19432974638189338\n",
      "Weight values:  [[ 3.76906249 -0.60327706 -0.20165916  1.62091366 -2.53851608]]\n",
      "Current Cost 0.18696701414834477\n",
      "Weight values:  [[ 4.12688591 -0.60267578 -0.2126287   1.82510108 -2.68221603]]\n",
      "Current Cost 0.18053932763523578\n",
      "Weight values:  [[ 4.4629805  -0.61044716 -0.23256804  1.98904909 -2.82741318]]\n",
      "Current Cost 0.17438096265210126\n",
      "Weight values:  [[ 4.77854604 -0.62283917 -0.25736472  2.12596268 -2.97041298]]\n",
      "Current Cost 0.1685373245906008\n",
      "Weight values:  [[ 5.07478636 -0.6375504  -0.28433183  2.24368642 -3.10933068]]\n",
      "Current Cost 0.1630975369715661\n",
      "Weight values:  [[ 5.35312198 -0.65324348 -0.3118518   2.34712213 -3.24334461]]\n",
      "Current Cost 0.15809903571909073\n",
      "Weight values:  [[ 5.61507146 -0.66916784 -0.33899952  2.43950235 -3.37220581]]\n",
      "Current Cost 0.15353760124621207\n",
      "Weight values:  [[ 5.86213209 -0.68491521 -0.3652738   2.52306407 -3.49596126]]\n",
      "Current Cost 0.14938642537012947\n",
      "Weight values:  [[ 6.095709   -0.70027271 -0.39042621  2.59941949 -3.61480392]]\n",
      "Current Cost 0.14560917782924618\n",
      "Weight values:  [[ 6.31708322 -0.71513774 -0.41435642  2.66977006 -3.72899239]]\n",
      "Current Cost 0.1421673389795489\n",
      "Weight values:  [[ 6.52740347 -0.72946934 -0.43704927  2.73503619 -3.83880815]]\n",
      "Current Cost 0.13902388455246384\n",
      "Weight values:  [[ 6.7276906  -0.74326054 -0.45853709  2.79593927 -3.94453309]]\n",
      "Current Cost 0.1361449111350077\n",
      "Weight values:  [[ 6.91884784 -0.7565227  -0.4788773   2.85305568 -4.0464381 ]]\n",
      "Current Cost 0.1335001700171805\n",
      "Weight values:  [[ 7.1016732  -0.76927652 -0.4981391   2.90685347 -4.1447777 ]]\n",
      "Current Cost 0.13106305477379443\n",
      "Weight values:  [[ 7.27687202 -0.78154708 -0.51639567  2.9577181  -4.23978808]]\n",
      "Current Cost 0.12881033804872358\n",
      "Weight values:  [[ 7.44506869 -0.7933609  -0.53371968  3.00597087 -4.33168676]]\n",
      "Current Cost 0.12672181272591374\n",
      "Weight values:  [[ 7.60681722 -0.80474444 -0.55018084  3.05188247 -4.42067336]]\n",
      "Current Cost 0.12477991593571647\n",
      "Weight values:  [[ 7.76261053 -0.81572321 -0.56584465  3.09568314 -4.50693067]]\n",
      "Current Cost 0.12296937311526587\n",
      "Weight values:  [[ 7.91288843 -0.82632136 -0.58077179  3.13757035 -4.59062601]]\n",
      "Current Cost 0.12127687761084233\n",
      "Weight values:  [[ 8.05804463 -0.83656156 -0.5950181   3.17771475 -4.67191259]]\n",
      "Current Cost 0.11969081017794467\n",
      "Weight values:  [[ 8.1984326  -0.84646486 -0.60863463  3.21626486 -4.75093081]]\n",
      "Current Cost 0.11820099730782521\n",
      "Weight values:  [[ 8.33437072 -0.85605084 -0.62166796  3.25335072 -4.82780946]]\n",
      "Current Cost 0.11679850491215582\n",
      "Weight values:  [[ 8.4661466  -0.8653376  -0.63416047  3.28908685 -4.90266691]]\n",
      "Current Cost 0.11547546307348254\n",
      "Weight values:  [[ 8.59402083 -0.87434191 -0.64615074  3.32357463 -4.97561207]]\n",
      "Current Cost 0.11422491752979706\n",
      "Weight values:  [[ 8.71823019 -0.88307927 -0.65767385  3.35690425 -5.04674537]]\n",
      "Current Cost 0.11304070388442586\n",
      "Weight values:  [[ 8.83899041 -0.89156405 -0.66876174  3.38915629 -5.11615953]]\n",
      "Current Cost 0.11191734099535908\n",
      "Weight values:  [[ 8.95649857 -0.89980956 -0.67944351  3.42040306 -5.18394031]]\n",
      "Current Cost 0.11084994048817956\n",
      "Weight values:  [[ 9.07093514 -0.90782816 -0.68974569  3.45070969 -5.25016719]]\n",
      "Current Cost 0.10983412979985992\n",
      "Weight values:  [[ 9.18246582 -0.91563131 -0.69969252  3.48013506 -5.31491395]]\n",
      "Current Cost 0.10886598657434825\n",
      "Weight values:  [[ 9.29124307 -0.92322968 -0.70930616  3.50873256 -5.37824916]]\n",
      "Current Cost 0.10794198258866788\n",
      "Weight values:  [[ 9.39740748 -0.9306332  -0.7186069   3.53655079 -5.4402367 ]]\n",
      "Current Cost 0.10705893569187895\n",
      "Weight values:  [[ 9.50108896 -0.93785113 -0.72761337  3.56363408 -5.50093612]]\n",
      "Current Cost 0.10621396849382256\n",
      "Weight values:  [[ 9.60240783 -0.94489212 -0.73634265  3.59002301 -5.56040307]]\n",
      "Current Cost 0.10540447275250092\n",
      "Weight values:  [[ 9.7014757  -0.95176425 -0.74481051  3.61575482 -5.6186896 ]]\n",
      "Current Cost 0.1046280785846008\n",
      "Weight values:  [[ 9.79839629 -0.95847507 -0.75303143  3.64086374 -5.67584446]]\n",
      "Current Cost 0.10388262776891956\n",
      "Weight values:  [[ 9.89326621 -0.96503167 -0.76101881  3.66538136 -5.73191341]]\n",
      "Current Cost 0.10316615053251063\n",
      "Weight values:  [[ 9.98617554 -0.9714407  -0.76878505  3.68933683 -5.7869394 ]]\n",
      "Current Cost 0.1024768453085473\n",
      "Weight values:  [[10.07720843 -0.9777084  -0.77634159  3.71275718 -5.84096286]]\n",
      "Current Cost 0.10181306103698681\n",
      "Weight values:  [[10.16644362 -0.98384063 -0.78369907  3.73566748 -5.89402183]]\n",
      "Current Cost 0.10117328164708661\n",
      "Weight values:  [[10.25395487 -0.98984291 -0.79086738  3.758091   -5.9461522 ]]\n",
      "Current Cost 0.10055611241723807\n",
      "Weight values:  [[10.33981142 -0.99572045 -0.79785569  3.78004947 -5.99738783]]\n",
      "Current Cost 0.09996026795450903\n",
      "Weight values:  [[10.4240783  -1.00147814 -0.80467256  3.8015631  -6.04776074]]\n",
      "Current Cost 0.09938456157537404\n",
      "Weight values:  [[10.50681671 -1.00712064 -0.81132597  3.82265081 -6.0973012 ]]\n",
      "Current Cost 0.09882789590180038\n",
      "Weight values:  [[10.58808429 -1.0126523  -0.81782337  3.84333031 -6.14603789]]\n",
      "Current Cost 0.09828925451420073\n",
      "Weight values:  [[10.6679354  -1.01807726 -0.82417173  3.86361818 -6.19399797]]\n",
      "Current Cost 0.09776769452575748\n",
      "Weight values:  [[10.74642139 -1.02339946 -0.83037758  3.88352998 -6.24120724]]\n",
      "Current Cost 0.09726233996195523\n",
      "Weight values:  [[10.82359077 -1.02862259 -0.83644704  3.90308036 -6.2876902 ]]\n",
      "Current Cost 0.09677237584549361\n",
      "Weight values:  [[10.89948947 -1.03375018 -0.84238584  3.92228308 -6.33347012]]\n",
      "Current Cost 0.09629704290055997\n",
      "Weight values:  [[10.97416099 -1.03878557 -0.84819938  3.94115112 -6.37856915]]\n",
      "Current Cost 0.09583563280216678\n",
      "Weight values:  [[11.04764658 -1.04373193 -0.85389273  3.95969672 -6.42300839]]\n",
      "Current Cost 0.09538748390622842\n",
      "Weight values:  [[11.11998538 -1.04859227 -0.85947069  3.97793146 -6.46680795]]\n",
      "Current Cost 0.09495197740454711\n",
      "Weight values:  [[11.19121461 -1.05336948 -0.86493774  3.99586626 -6.50998698]]\n",
      "Current Cost 0.09452853385614521\n",
      "Weight values:  [[11.26136963 -1.05806627 -0.87029815  4.01351148 -6.55256381]]\n",
      "Current Cost 0.09411661005260943\n",
      "Weight values:  [[11.3304841  -1.06268526 -0.87555593  4.03087693 -6.5945559 ]]\n",
      "Current Cost 0.09371569618044791\n",
      "Weight values:  [[11.39859008 -1.06722891 -0.88071488  4.04797192 -6.63597997]]\n",
      "Current Cost 0.09332531324807454\n",
      "Weight values:  [[11.46571816 -1.07169961 -0.8857786   4.06480528 -6.676852  ]]\n",
      "Current Cost 0.09294501074899905\n",
      "Weight values:  [[11.53189748 -1.0760996  -0.89075048  4.08138541 -6.71718729]]\n",
      "Current Cost 0.0925743645362318\n",
      "Weight values:  [[11.59715589 -1.08043105 -0.89563377  4.0977203  -6.75700051]]\n",
      "Current Cost 0.09221297488589597\n",
      "Weight values:  [[11.66152    -1.08469601 -0.90043151  4.11381755 -6.7963057 ]]\n",
      "Current Cost 0.09186046473060984\n",
      "Weight values:  [[11.72501526 -1.08889647 -0.90514663  4.12968441 -6.83511634]]\n",
      "Current Cost 0.09151647804546659\n",
      "Weight values:  [[11.78766599 -1.0930343  -0.90978189  4.14532779 -6.87344535]]\n",
      "Current Cost 0.09118067837138072\n",
      "Weight values:  [[11.84949551 -1.09711132 -0.91433992  4.16075428 -6.91130515]]\n",
      "Current Cost 0.09085274746230085\n",
      "Weight values:  [[11.91052616 -1.10112926 -0.91882323  4.17597018 -6.94870767]]\n",
      "Current Cost 0.09053238404426833\n",
      "Weight values:  [[11.97077932 -1.10508978 -0.9232342   4.19098152 -6.98566438]]\n",
      "Current Cost 0.09021930267564057\n",
      "Weight values:  [[12.03027555 -1.10899446 -0.9275751   4.20579405 -7.02218632]]\n",
      "Current Cost 0.08991323269892144\n",
      "Weight values:  [[12.08903456 -1.11284485 -0.93184812  4.22041328 -7.05828408]]\n",
      "Current Cost 0.08961391727568982\n",
      "Weight values:  [[12.14707528 -1.1166424  -0.93605531  4.2348445  -7.0939679 ]]\n",
      "Current Cost 0.0893211124969894\n",
      "Weight values:  [[12.20441591 -1.12038853 -0.94019867  4.24909275 -7.12924763]]\n",
      "Current Cost 0.08903458656234978\n",
      "Weight values:  [[12.26107394 -1.12408459 -0.94428008  4.2631629  -7.16413274]]\n",
      "Current Cost 0.0887541190212995\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight values:  [[12.31706619 -1.12773188 -0.94830135  4.27705959 -7.19863239]]\n",
      "Current Cost 0.08847950007185859\n",
      "Weight values:  [[12.37240888 -1.13133166 -0.95226423  4.2907873  -7.2327554 ]]\n",
      "Current Cost 0.08821052991104616\n",
      "Weight values:  [[12.4271176  -1.13488513 -0.95617036  4.30435033 -7.2665103 ]]\n",
      "Current Cost 0.08794701813293074\n",
      "Weight values:  [[12.48120738 -1.13839345 -0.96002134  4.31775279 -7.29990531]]\n",
      "Current Cost 0.08768878317018063\n",
      "Weight values:  [[12.53469271 -1.14185775 -0.96381869  4.33099867 -7.33294838]]\n",
      "Current Cost 0.08743565177547011\n",
      "Weight values:  [[12.58758757 -1.1452791  -0.96756388  4.34409178 -7.36564718]]\n",
      "Current Cost 0.08718745853943688\n",
      "Weight values:  [[12.63990543 -1.14865854 -0.97125832  4.3570358  -7.39800914]]\n",
      "Current Cost 0.08694404544219538\n",
      "Weight values:  [[12.69165932 -1.15199707 -0.97490334  4.36983428 -7.43004143]]\n",
      "Current Cost 0.0867052614356988\n",
      "Weight values:  [[12.74286179 -1.15529566 -0.97850026  4.38249062 -7.46175101]]\n",
      "Current Cost 0.08647096205447656\n",
      "Weight values:  [[12.793525   -1.15855525 -0.98205031  4.39500812 -7.4931446 ]]\n",
      "Current Cost 0.08624100905251204\n",
      "Weight values:  [[12.84366067 -1.16177674 -0.98555469  4.40738996 -7.5242287 ]]\n",
      "Current Cost 0.08601527006421454\n",
      "Weight values:  [[12.89328015 -1.16496099 -0.98901457  4.41963921 -7.55500963]]\n",
      "Current Cost 0.0857936182876289\n",
      "Weight values:  [[12.94239443 -1.16810886 -0.99243104  4.43175882 -7.58549349]]\n",
      "Current Cost 0.08557593218818732\n",
      "Weight values:  [[12.99101411 -1.17122115 -0.99580517  4.44375165 -7.6156862 ]]\n",
      "Current Cost 0.0853620952214481\n",
      "Weight values:  [[13.03914948 -1.17429865 -0.99913801  4.45562046 -7.64559352]]\n",
      "Current Cost 0.08515199557340966\n",
      "Weight values:  [[13.08681051 -1.17734213 -1.00243055  4.46736793 -7.675221  ]]\n",
      "Current Cost 0.08494552591710204\n",
      "Weight values:  [[13.13400682 -1.18035232 -1.00568373  4.47899664 -7.70457405]]\n",
      "Current Cost 0.08474258318426438\n",
      "Weight values:  [[13.18074777 -1.18332993 -1.0088985   4.49050909 -7.73365792]]\n",
      "Current Cost 0.08454306835102354\n",
      "Weight values:  [[13.22704242 -1.18627566 -1.01207573  4.50190769 -7.7624777 ]]\n",
      "Current Cost 0.08434688623657421\n",
      "Weight values:  [[13.27289955 -1.18919018 -1.0152163   4.51319479 -7.79103834]]\n",
      "Current Cost 0.0841539453139344\n",
      "Weight values:  [[13.31832769 -1.19207414 -1.01832104  4.52437266 -7.81934464]]\n",
      "Current Cost 0.08396415753194031\n",
      "Weight values:  [[13.36333511 -1.19492816 -1.02139074  4.53544349 -7.84740127]]\n",
      "Current Cost 0.08377743814770003\n",
      "Weight values:  [[13.40792984 -1.19775285 -1.0244262   4.54640942 -7.87521277]]\n",
      "Current Cost 0.08359370556878633\n",
      "Weight values:  [[13.45211966 -1.20054881 -1.02742817  4.55727251 -7.90278356]]\n",
      "Current Cost 0.08341288120451101\n",
      "Weight values:  [[13.49591216 -1.20331661 -1.03039736  4.56803476 -7.93011791]]\n",
      "Current Cost 0.08323488932567423\n",
      "Weight values:  [[13.53931468 -1.20605682 -1.0333345   4.57869812 -7.95722002]]\n",
      "Current Cost 0.08305965693221912\n",
      "Weight values:  [[13.58233438 -1.20876995 -1.03624025  4.58926448 -7.98409394]]\n",
      "Current Cost 0.08288711362827576\n",
      "Cost Chart[indexed by 'iteration']:  [0.2647917  0.23631339 0.20687076 0.19432975 0.18696701 0.18053933\n",
      " 0.17438096 0.16853732 0.16309754 0.15809904 0.1535376  0.14938643\n",
      " 0.14560918 0.14216734 0.13902388 0.13614491 0.13350017 0.13106305\n",
      " 0.12881034 0.12672181 0.12477992 0.12296937 0.12127688 0.11969081\n",
      " 0.118201   0.1167985  0.11547546 0.11422492 0.1130407  0.11191734\n",
      " 0.11084994 0.10983413 0.10886599 0.10794198 0.10705894 0.10621397\n",
      " 0.10540447 0.10462808 0.10388263 0.10316615 0.10247685 0.10181306\n",
      " 0.10117328 0.10055611 0.09996027 0.09938456 0.0988279  0.09828925\n",
      " 0.09776769 0.09726234 0.09677238 0.09629704 0.09583563 0.09538748\n",
      " 0.09495198 0.09452853 0.09411661 0.0937157  0.09332531 0.09294501\n",
      " 0.09257436 0.09221297 0.09186046 0.09151648 0.09118068 0.09085275\n",
      " 0.09053238 0.0902193  0.08991323 0.08961392 0.08932111 0.08903459\n",
      " 0.08875412 0.0884795  0.08821053 0.08794702 0.08768878 0.08743565\n",
      " 0.08718746 0.08694405 0.08670526 0.08647096 0.08624101 0.08601527\n",
      " 0.08579362 0.08557593 0.0853621  0.085152   0.08494553 0.08474258\n",
      " 0.08454307 0.08434689 0.08415395 0.08396416 0.08377744 0.08359371\n",
      " 0.08341288 0.08323489 0.08305966 0.08288711]\n"
     ]
    }
   ],
   "source": [
    "#Train the Neural Network\n",
    "iterations = 100\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "#Total cost\n",
    "J_total = 0\n",
    "\n",
    "#Matrix of weights for a perceptron, NX1\n",
    "scale = 1\n",
    "w = np.random.randn(1,x.shape[0])*scale\n",
    "while(w[w == 0] is True):\n",
    "    w = np.random.randn(1,x.shape[0])*scale\n",
    "print(\"Weight shape:\", w.shape)\n",
    "print(\"Weight Values:\",w)\n",
    "\n",
    "cost_total = np.empty((0,0))\n",
    "weight_evolution = np.empty((0,w.shape[1]))\n",
    "\n",
    "#Neural Network W/ Quantum Gradient Descent (AQC QNN)\n",
    "#This function is AT LEAST on the order of O(n^4) \n",
    "#We can embrace symmetry and modify the weights \n",
    "#to reach better costs values & reduce the time complexity\n",
    "for k in range(iterations):\n",
    "    c = 0\n",
    "    for training_ex in range(y.shape[1]):\n",
    "        #print(\"Training example #\",training_ex)\n",
    "        for training_feat in range(x.shape[0]):\n",
    "            #Prevent log(0) errors\n",
    "            #Forward Propagation\n",
    "            f = np.dot(w,x)\n",
    "            a = sigmoid(f)\n",
    "            #print(\"Changing Cost:\", c)\n",
    "            z_x = np.array([[1,0],[0,-1]])\n",
    "            H_a = np.array([[a[0,training_ex],0],[0,a[0,training_ex]]])\n",
    "            if(y[:,training_ex] == 1):\n",
    "                H_y = np.identity(2)\n",
    "            else:\n",
    "                H_y=np.zeros((2,2))\n",
    "            H_p = np.dot((H_y-H_a),z_x)\n",
    "            H_p = H_p.reshape((H_p.shape[0],H_p.shape[1],1))\n",
    "\n",
    "            #SOLVE FOR MINIMUM GRADIENT VIA AQC\n",
    "            H_t = (1-t/T)*H_0 + t/T*H_p\n",
    "            psi_t, psi_f = solve_comp(ode_ham, H_t, psi_0, t)\n",
    "            expect = np.dot(\n",
    "            np.matrix.getH(psi_f),np.dot(\n",
    "            H_t[:,:,H_t.shape[2]-1],psi_f))\n",
    "            if(x[training_feat,training_ex] == y[:,training_ex]-1 or x[training_feat,training_ex]==y[:,training_ex]):\n",
    "                #Smallest gradient\n",
    "                weight_evolution = np.append(weight_evolution, w[:,training_feat])\n",
    "                w[:,training_feat] += np.real(expect)\n",
    "            else:\n",
    "                #Reverse Polarity\n",
    "                weight_evolution = np.append(weight_evolution, w[:,training_feat])\n",
    "                w[:,training_feat] -= np.real(expect)\n",
    "            weight_evolution = np.append(weight_evolution,w[:,training_feat])\n",
    "                                          \n",
    "    #Take the cost of the training example if you would like to see how it changes per training example\n",
    "    c = -1/x.shape[1]*(y*np.log(a)+(1-y)*np.log(1-a))\n",
    "    #Sometimes, our prediction can be SO accurate that we end up computing np.log(0)\n",
    "    #We check for these answers and modify it to zero instead\n",
    "    #print(\"Before nan/inf checks: \",c)\n",
    "    c[np.isinf(c)] = 0\n",
    "    c[np.isnan(c)] = 0\n",
    "    #print(\"After nan/inf checks: \",c)\n",
    "    \n",
    "    cost_total = np.append(cost_total,np.sum(c))\n",
    "    if k %1 == 0:\n",
    "        print(\"Weight values: \", w)\n",
    "        print(\"Current Cost\", cost_total[k])\n",
    "print(\"Cost Chart[indexed by 'iteration']: \",cost_total)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d9e1d0",
   "metadata": {},
   "source": [
    "At this point we want to observe the cost function over every iteration to determine that we have in-fact created a learning algorithm via AQC simulation; we do so by plotting the cost function vs iteration graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9c843773",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(range(iterations), cost_total, '.-r')\n",
    "plt.title(\"Iterations Vs. Cost\")\n",
    "plt.xlabel(\"Iterations\")\n",
    "plt.ylabel(\"Cost\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba348000",
   "metadata": {},
   "source": [
    "## Let's test some values:\n",
    "Suppose we had a \"test\" set (for this data set it's every $2^5$ combination):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5e48ec6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[13.58233438 -1.20876995 -1.03624025  4.58926448 -7.98409394]]\n",
      "[[-3.55255841e-04 -3.99859010e-07 -1.49109043e+00 -4.12783141e-11\n",
      "  -4.47241211e-05 -5.03315280e-08 -3.59942810e-01 -5.19584376e-12\n",
      "  -3.16728579e-05 -3.56436953e-08 -2.67593974e-01 -3.67961217e-12\n",
      "  -3.98681807e-06 -4.48658478e-09 -3.78928659e-02 -4.63185046e-13\n",
      "  -4.48658489e-09 -3.98681807e-06 -4.63185046e-13 -3.78928659e-02\n",
      "  -3.56436954e-08 -3.16728579e-05 -3.67950115e-12 -2.67593974e-01\n",
      "  -5.03315281e-08 -4.47241211e-05 -5.19584376e-12 -3.59942810e-01\n",
      "  -3.99859010e-07 -3.55255841e-04 -4.12783141e-11 -2.55056180e-01]]\n",
      "predictions:  [[0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      "  1. 1. 1. 1. 1. 1. 1. 0.]]\n",
      "Labels:  [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      "  1. 1. 1. 1. 1. 1. 1. 0.]]\n",
      "Accuracy:  96.875\n"
     ]
    }
   ],
   "source": [
    "print(w)\n",
    "#We can test with a single set of values, but since there are only 2^5 possiblities, we can just test using\n",
    "#Our training set\n",
    "f = np.dot(w,x)\n",
    "a = sigmoid(f)\n",
    "c = y*np.log(a) + (1-y)*np.log(1-a)\n",
    "print(c)\n",
    "predictions = sigmoid(f)\n",
    "predictions[predictions>=0.5] = 1\n",
    "predictions[predictions<0.5] = 0\n",
    "print(\"predictions: \", predictions)\n",
    "print(\"Labels: \", y)\n",
    "\n",
    "accuracy = (1- np.sum(np.abs(predictions-y))/y.shape[1])*100\n",
    "print(\"Accuracy: \",accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "979e8d82",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "If we play with the values of our input and their respective labels, the cost function decreases all the same. There are some (possible) issues with this model that are not unlike classical versions:\n",
    "\n",
    "1. The model sometimes experienced the \"vanishing gradient\" issue (large values of w_i)\n",
    "2. The model does not explore more layers\n",
    "3. The model does not include a bias\n",
    "4. The model does not include regularization\n",
    "5. The runtime of the model is incredibly long for a small dataset\n",
    "\n",
    "The solution for the first issue can be solved via `hyperparameter tuning`, so this is not a problem. $2-4$ are left to be explored since this algorithm only lays the ground work for a deep and thorough neural network algorithm. $5$ might be remidied by implementing AQC on a real quantum neural network.\n",
    "\n",
    "\n",
    "One big takeaway from this project is that it seems feasible for a quantum computer to compute the proper gradients for a neural network. Our model would be equivalent to a perceptron with $5$ input nodes. We successfully replaced back-propagation via adiabatic quantum computing and identified a majority of the test/training set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1802bfd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
